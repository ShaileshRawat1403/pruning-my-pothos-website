---
title: "Evaluation and prompting references"
description: "Shortlist for building safer, more measurable prompts."
publishDate: "2026-01-21"
tags:
  - resources
  - evaluation
  - prompting
  - safety
coverUrl: "/covers/shelf/shared-eval.svg"
---

- [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) — A practical baseline for comparing model behavior.
- [OpenAI Evals](https://github.com/openai/evals) — Useful patterns for building custom evals.
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903) — A clear framing for multi-step reasoning prompts.

