---
title: "Retrieval-Augmented Generation in Plain Terms"
description: "How retrieval grounds outputs and where it can still fail."
category: "How-things-fit-together"
tags:
  - rag
  - retrieval
  - grounding
  - reliability
---

Retrieval-Augmented Generation (RAG) is a technique that connects a Large Language Model to an external knowledge source. It allows the model to generate answers that are grounded in specific, up-to-date, or private information, rather than relying solely on its static training data.

<div id="toc-anchor"></div>
<nav class="toc" aria-label="On-page">
  <h2 class="toc-title">Contents</h2>
  <div class="toc-groups">
    <details open>
      <summary>Act I: The fundamentals</summary>
      <ol>
        <li><a href="#retriever-plus-generator">Retriever + generator</a></li>
      </ol>
    </details>
    <details>
      <summary>Act II: The modern paradigm</summary>
      <ol>
        <li><a href="#the-rag-pipeline">The RAG pipeline</a></li>
      </ol>
    </details>
    <details>
      <summary>Act III: Principles in practice</summary>
      <ol>
        <li><a href="#failure-modes-and-data-quality">Failure modes and data quality</a></li>
        <li><a href="#what-this-changes-in-practice">What this changes in practice</a></li>
      </ol>
    </details>
  </div>
</nav>

## Act I: The fundamentals

### Retriever + generator

A standard LLM's knowledge is frozen at the end of its training. It knows nothing about events that have happened since, nor does it have access to your company's private documents. This leads to "hallucinations" or factually incorrect answers when asked about things outside its knowledge base.

RAG addresses this by combining two systems:
1.  **A Retriever:** A search system that can find relevant information from a specified knowledge base (like a collection of documents, a database, or a website).
2.  **A Generator:** A standard LLM that takes the retrieved information and uses it to synthesize a human-readable answer.

The retriever's job is to find the right puzzle pieces; the generator's job is to assemble them.

<figure class="diagram">
  <svg viewBox="0 0 900 300" role="img" aria-labelledby="rag-flow-title rag-flow-desc">
    <title id="rag-flow-title">RAG Pipeline</title>
    <desc id="rag-flow-desc">A diagram showing a user query first going to a retriever, which pulls from a knowledge base, and then the augmented prompt is sent to an LLM to generate the final answer.</desc>
    <text x="40" y="80" font-size="13" font-family="var(--font-mono)">1. Query</text>
    <rect x="40" y="90" width="150" height="60" rx="10" fill="none" stroke="currentColor" stroke-width="2" />
    <text x="60" y="128" font-size="13" font-family="var(--font-mono)">User Question</text>
    <path d="M 190 120 H 290" stroke="currentColor" stroke-width="2" />
    <path d="M 280 120 L 290 115 L 290 125 Z" fill="currentColor" />
    <text x="300" y="80" font-size="13" font-family="var(--font-mono)">2. Retrieve</text>
    <rect x="300" y="90" width="150" height="60" rx="10" fill="none" stroke="currentColor" stroke-width="2" />
    <text x="320" y="128" font-size="13" font-family="var(--font-mono)">Search Docs</text>
    <path d="M 375 150 V 200" stroke="currentColor" stroke-width="2" />
    <path d="M 375 190 L 370 200 L 380 200 Z" fill="currentColor" />
    <rect x="300" y="210" width="150" height="60" rx="10" fill="none" stroke="currentColor" stroke-width="2" stroke-dasharray="5 5" />
    <text x="310" y="248" font-size="13" font-family="var(--font-mono)">Knowledge Base</text>
    <path d="M 450 120 H 550" stroke="currentColor" stroke-width="2" />
    <path d="M 540 120 L 550 115 L 550 125 Z" fill="currentColor" />
    <text x="560" y="80" font-size="13" font-family="var(--font-mono)">3. Augment & 4. Generate</text>
    <rect x="560" y="90" width="280" height="80" rx="10" fill="none" stroke="var(--color-accent)" stroke-width="2" />
    <text x="580" y="120" font-size="13" font-family="var(--font-mono)">Prompt + Retrieved Docs</text>
    <text x="650" y="150" font-size="13" font-family="var(--font-mono)">=> LLM</text>
  </svg>
  <figcaption>The RAG process: retrieve relevant context, augment the prompt, and then generate the answer.</figcaption>
</figure>

## Act II: The modern paradigm

### The RAG pipeline

The standard RAG pipeline works as follows:
1.  **Indexing:** An external knowledge base (e.g., PDFs, web pages, Notion docs) is broken into chunks, and each chunk is converted into a numerical embedding. These embeddings are stored in a vector database.
2.  **Retrieval:** When a user asks a question, the question is also converted into an embedding. The vector database is searched for the document chunks with the most similar embeddings. These are the "retrieved documents."
3.  **Augmentation:** The original question and the retrieved documents are combined into a new, augmented prompt. The prompt might look something like this: `"Given the following context documents, please answer the user's question. Context: [retrieved documents]. Question: [original question]."`
4.  **Generation:** This augmented prompt is sent to an LLM, which generates an answer based on the provided context.

This process ensures that the model's answer is directly informed by the external data, not just its internal training.

## Act III: Principles in practice

### Failure modes and data quality

RAG is a powerful technique, but it is not a magic bullet. Its effectiveness is highly dependent on the quality of the retriever. If the retriever fails to find the correct documents, the generator will not have the information it needs to produce a correct answer. This is the principle of "garbage in, garbage out."

Common failure modes include:
- **Poor data quality:** The knowledge base contains inaccurate or outdated information.
- **Chunking problems:** Documents are split in ways that separate related ideas, making it hard to retrieve full context.
- **Retrieval mismatch:** The user's question is phrased in a way that does not match the language of the documents, leading the semantic search to fail.

Therefore, building a RAG system is not just about connecting an LLM to a database. It is about carefully curating the knowledge base, optimizing the retrieval process, and implementing checks to handle cases where no relevant information is found.

### What this changes in practice

Instead of just prompting a model, you must first ensure it has access to the right information by building a reliable retrieval system.
